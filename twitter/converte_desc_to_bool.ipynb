{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/data+descvf.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_description  am bot that tweets about sports and technology follow me for updates\n",
      "doc  am bot that tweets about sports and technology follow me for updates\n",
      "[' ', 'bot', 'tweet', 'sport', 'technology', 'follow', 'update']\n",
      "pos count Counter({'NOUN': 4, 'PRON': 2, 'VERB': 2, 'ADP': 2, 'SPACE': 1, 'AUX': 1, 'CCONJ': 1})\n",
      "Profile description: I am a bot that tweets about sports and technology. Follow me for updates!\n",
      "Is bot? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def is_bot_profile(profile_description):\n",
    "    # remove emojis\n",
    "    profile_description = TwitterPreprocessor(str(profile_description)).desc_preprocess().text\n",
    "    print(\"profile_description\",profile_description)\n",
    "    # tokenize the text with spaCy\n",
    "    doc = nlp(profile_description)\n",
    "    print(\"doc\",doc)\n",
    "    # remove stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "    print(tokens)\n",
    "    # count the frequency of different parts-of-speech\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    print(\"pos count\",pos_counts)\n",
    "    # look for patterns typical of bot accounts\n",
    "    if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "        return True\n",
    "    elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#  \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "profile_description =\"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "is_bot = is_bot_profile(profile_description)\n",
    "print(f\"Profile description: {profile_description}\")\n",
    "print(f\"Is bot? {is_bot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a bot that tweets about sports and technology. follow me for updates!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.morphology.Morphology' object has no attribute 'tag_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m profile_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am a bot that tweets about sports and technology. Follow me for updates!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m is_bot \u001b[39m=\u001b[39m is_bot_profile(profile_description)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProfile description: \u001b[39m\u001b[39m{\u001b[39;00mprofile_description\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIs bot? \u001b[39m\u001b[39m{\u001b[39;00mis_bot\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mis_bot_profile\u001b[1;34m(profile_description)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mmorphology\u001b[39m.\u001b[39mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mmorphology\u001b[39m.\u001b[39;49mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.morphology.Morphology' object has no attribute 'tag_map'"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# import emoji\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def is_bot_profile(profile_description):\n",
    "#     # remove emojis\n",
    "#     # profile_description = emoji.get_emoji_regexp().sub(r'', profile_description)\n",
    "#     # lowercase the text\n",
    "#     profile_description = profile_description.lower()\n",
    "#     # tokenize the text with spaCy\n",
    "#     doc = nlp(profile_description)\n",
    "#     print(doc)\n",
    "#     # remove stop words and punctuation\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "#     print(tokens)\n",
    "#     # count the frequency of different parts-of-speech\n",
    "#     pos_tags = [token.pos_ for token in doc]\n",
    "#     print(pos_tags)\n",
    "#     pos_counts = Counter(pos_tags)\n",
    "#     # look for patterns typical of bot accounts\n",
    "#     if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "#         return True\n",
    "#     elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# profile_description = \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "# is_bot = is_bot_profile(profile_description)\n",
    "# print(f\"Profile description: {profile_description}\")\n",
    "# print(f\"Is bot? {is_bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chams\\AppData\\Local\\Temp\\ipykernel_16676\\774891700.py:7: DtypeWarning: Columns (0,3,4,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# import spacy\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n",
    "\n",
    "# # Clean the text data\n",
    "# dataset['description'] = dataset['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "# dataset['description'] = dataset['description'].fillna('')\n",
    "# # Define function to lemmatize the tokens\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# def lemmatize_tokens(tokens):\n",
    "#     doc = nlp(\" \".join(tokens))\n",
    "#     return [token.lemma_ for token in doc]\n",
    "\n",
    "# # Tokenize and lemmatize the text data\n",
    "# dataset['description_tokens'] = dataset['description'].apply(lambda x: lemmatize_tokens(word_tokenize(x)))\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# dataset.to_csv('./Data/description_tokens.csv', index=False)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Convert the list of tokens to a string\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the dense array to the dataset\n",
    "# dataset['description_tokens'] = X_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('./Data/description_tokensv2.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start prepare desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "# # Remove irrelevant or duplicated data\n",
    "# df = df.drop(['description',  \n",
    "#          'account_type'], axis=1)\n",
    "\n",
    "# # Remove special characters and punctuation, and convert text to lowercase\n",
    "# df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "\n",
    "# # Tokenize text\n",
    "# df['description'] = df['description'].apply(lambda x: x.split())\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['description'] = df['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# # Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['description'] = df['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# # Convert back to string\n",
    "# df['description'] = df['description'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# # Split into training and testing sets\n",
    "# train_size = int(0.8 * len(df))\n",
    "# train_data = df[:train_size]\n",
    "# test_data = df[train_size:]\n",
    "\n",
    "# # Tokenize text inputs\n",
    "# tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['description'])\n",
    "\n",
    "# # Convert text inputs to sequences of integers\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['description'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['description'])\n",
    "\n",
    "# # Pad sequences\n",
    "# max_len = 100\n",
    "# train_data = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "# test_data = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential([\n",
    "#     Embedding(10000, 128, input_length=max_len),\n",
    "#     LSTM(64),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Fit model\n",
    "# history = model.fit(train_data, train_data['label'], epochs=10, validation_data=(test_data, test_data['label']))\n",
    "\n",
    "# # Evaluate model\n",
    "# loss, accuracy = model.evaluate(test_data, test_data['label'])\n",
    "# print(f'Test loss: {loss:.4f}')\n",
    "# print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        television producer emmy award winner disney e...\n",
       "2                                          self discovery \n",
       "4        productor de televisiÃ³n embajador de contacto ...\n",
       "6        im dmstic engineer married to hnk for yrs pups...\n",
       "7        shenanigatrix wordsmith events social media co...\n",
       "                               ...                        \n",
       "34039    self made plus model ceo siswim world traveler...\n",
       "34040                                   gazetecijournalist\n",
       "34041    modelo actriz venezolana venezuelan model actr...\n",
       "34042                              indianactorneed no more\n",
       "34043    poeta autor escritor acadÃªmico dois livros pub...\n",
       "Name: description, Length: 27034, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "\n",
    "# download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "\n",
    "# Remove special characters and punctuation, and convert text to lowercase\n",
    "df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).partially_preprocess().text)\n",
    "df = df.dropna(subset=['description'])\n",
    "# example list of string descriptions\n",
    "descriptions = df['description'] \n",
    "\n",
    "# create list of stopwords to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize list of lemmatized tokens for each description\n",
    "lemmatized_tokens_list = []\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "\n",
    "descriptions.replace(\"nan\", nan_value, inplace=True)\n",
    "descriptions=descriptions.dropna()\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Â¯Â¯\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "ð™„ð™£ð™¨ð™©ð™–ð™œð™§ð™–ð™¢ ð™¤ð™¡ð™–ð™Ÿð™¤ð™§ð™™ð™–ð™£ â€¢ ð™Žð™£ð™–ð™¥ð™˜ð™ð™–ð™© ð™¤ð™¡ð™–ð™Ÿð™¤ð™§ð™™ð™–ð™£ â€¢ ð™ˆð™šð™™ð™žð™– ð™šð™£ð™¦ð™ªð™žð™§ð™žð™šð™¨ ð™˜ð™¤ð™£ð™©ð™–ð™˜ð™© \n",
      " \n",
      "\n",
      " \n",
      "ð˜Šð˜©ð˜¢ð˜®ð˜±ð˜ªð˜°ð˜¯ ð˜¥ð˜¶ ð˜”ð˜°ð˜¯ð˜¥ð˜¦ ð˜ð˜Š ð˜‰ð˜¢ð˜³ð˜¤ð˜¦ð˜­ð˜°ð˜¯ð˜¢ ð˜¤ð˜°ð˜¯ð˜µð˜¢ð˜¤ð˜µð˜¤ð˜°ð˜®\n",
      " \n",
      "\n",
      "â€œð‘³ð’†ð’•ð’•ð’Šð’ð’ˆ ð’‘ð’†ð’ð’‘ð’ð’† ð’…ð’ð’˜ð’ ð’Šð’” ð’Žð’š ð’•ð’‰ð’Šð’ð’ˆ ð’ƒð’‚ð’ƒð’š â€” ð’‡ð’Šð’ð’… ð’šð’ð’–ð’“ð’”ð’†ð’ð’‡ ð’ð’†ð’˜ ð’ˆð’Šð’ˆâ€ \n",
      "\n",
      "\n",
      "ð—•ð—²ð—›ð—®ð—½ð—½ð˜†ð—œð˜ð——ð—¿ð—¶ð˜ƒð—²ð˜€ð—£ð—²ð—¼ð—½ð—¹ð—²ð—–ð—¿ð—®ð˜‡ð˜† \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " â™¡\n",
      "\n",
      " \n",
      "â˜†\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " â€ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "à¸±\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " â˜¥ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â—žâ€¸â—Ÿâ˜ž ð™·ðšŽðšŠðšŸðšŽðš—â€™ ð™¶ðšŠðšðšŽ ðšðš›ðš˜ðš™ðš˜ðšžðš \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â€¢â€¢\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "â€¢â€¢ \n",
      "\n",
      " â— â€¿â—  \n",
      " \n",
      "\n",
      " \n",
      " ð”±ð”¥ð”¢ ð”¬ð”«ð”¢ ð”žð”«ð”¡ ð”¬ð”«ð”©ð”¶ ð”©ð”¦ð”³ð”¦ð”«ð”¤ ð”¡ð”¢ð” ð”¢ð”žð”°ð”¢ð”¡ \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ð’¯ð’½â„¯ ð’Ÿð’¾ð“‹ð’¾ð“ƒâ„¯ â„±â„¯ð“‚ð’¾ð“ƒð’¾ð“ƒâ„¯ â¥\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "â˜†ã€‚\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "ð€ð“ð‹ð€ð’ ð€ð¥ð¢ð¬ \n",
      "\n",
      "\n",
      "â™¡â™¡\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ð‘…ð‘’ð“ˆð’¾ð“ˆð“‰ð’¶ð“ƒð’¸ð‘’ ð¿ð’¾ð’·ð‘’ð“‡ð’¶ð“ ð’Ÿð‘’ð“‚ ðµð“ð“Šð‘’ ð’²ð’¶ð“‹ð‘’ ð’®ð“ƒð‘œð“Œð’»ð“ð’¶ð“€ð‘’ ð’±ð‘œð“‰ð‘’ðµð“ð“Šð‘’ð’©ð‘œð‘€ð’¶ð“‰ð“‰ð‘’ð“‡ð’²ð’½ð‘œ ð’Ÿð‘€â€™\n",
      "\n",
      "â˜† \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "ð•½ð–Šð–†ð–‘ ð–Žð–˜ ð•½ð–†ð–—ð–Š \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "â€ \n",
      "\n",
      "\n",
      "Â¯Â¯\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ï½…ï½Œï½…ï½ƒï½”ï½‰ï½ï½Ž ï½ƒï½ï½–ï½…ï½’ï½ï½‡ï½…\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â–³âƒ’âƒ˜ âš¯Í› â€¢ â€¢ \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "ï¼ï¼ï¼\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â™¡ â™¡\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "â™¡\n",
      "ð“ð¡ðž ð–ðšð¥ð¤ð¢ð§ð  ðƒðžðšð ð‚ð€ð‘ð˜ð‹ ðºð‘’ð‘¡ ð‘œð‘› ð‘¡â„Žð‘’ ð‘ð‘–ð‘˜ð‘’ ð‘Žð‘›ð‘‘ ð‘”ð‘œ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "ð–Œð–”ð–™ð– ð–†ð–“ð–Œð–Šð–‘ ð–˜ð–Žð–“ð–“ð–Šð–—\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï½¡ï½¥ï½¥â˜…ï½¡\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "ð‘ƒð‘£ð‘Ÿð‘ð‘™ð‘’ð‘€ð‘¥ð‘ð¿ð‘™ð‘ ð‘¬ð’”ð— ðŸðŸŽðŸðŸ\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ð—€ð—‚ð—‹ð—… ð–»ð—ˆð—Œð—Œ ð—Œð—ð—ˆð—‹ð—’ð—…ð—‚ð—‡ð–¾ ð—ƒð–¾ð—ð–¾ð—…ð—‹ð—’\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ð–¨ð–¦ ð–½ð—‹ð–¾ð–¿ð–ºð–¼ð–¾ â€¢ ð–²ð–¢ ð– ð—‡ð–½ð—‹ð–¾ð–º ð–£ð–ºð—ð—‚ð—Œ â€¢ ð–µð—‚ð—‹ð—€ð—ˆ â€¢ â€¢ \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "à¿Š\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ð“•ð“ªð“¼ð“±ð“²ð“¸ð“· ð““ð“®ð“¼ð“²ð“°ð“·ð“®ð“»\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ð‚ð¡ð¥ð¨ðž ðˆð§ðð¢ðš\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "âœ­ â˜†\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ð••ð• ð•Ÿð•¥ ð•£ð•’ð•¡ ð•›ð•¦ð•¤ð•¥ ð•¥ð•’ð•ð•œ ð•ð• ð•¥ ð•’ð•Ÿð•• ð•¤ð• ð•žð•–ð•¥ð•šð•žð•–ð•¤ ð•šð•¥ ð•˜ð•–ð•¥ð•¤ ð•£ð•–ð•”ð• ð•£ð••ð•–ð••\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "â“¥ \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "â€” ð‘Žð‘›ð‘–ð‘ ð‘¡ð‘œð‘› ð‘‘ð‘’ð‘šð‘ð‘ ð‘’ð‘¦ ð‘ð‘œð‘šð‘ð‘’ð‘œâ €â €â €â €â €â €â €â € â €â €â € â €â €â € â €â €â €â €â €â €â €â €â € ð‘“ð‘Žð‘› ð‘Žð‘ð‘ð‘œð‘¢ð‘›ð‘¡\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ð‘´ð’‚ð’Œð’†ð’–ð’‘ ð‘¨ð’“ð’•ð’Šð’”ð’• ð‘·ð’‰ð’ð’•ð’ð’ˆð’“ð’‚ð’‘ð’‰ð’†ð’“\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ï½€âˆ‡Â´\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â˜‰â­¡ â˜½ \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ð™ð™‘ ð™‹ð™§ð™¤ð™™ð™ªð™˜ð™šð™§ ð™‹ð™¤ð™™ð™˜ð™–ð™¨ð™© ð™ƒð™¤ð™¨ð™© ð™¤ð™› ð™…ð™¤ð™£ð™–ð™ ð™–ð™£ð™™ ð™©ð™ð™š ð™’ð™ð™–ð™¡ð™š\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "ðˆð† ð…ð ð‚ð«ð¢ð¬ð­ð¢ð§ðž ð‘ðžð²ðžð¬ âˆ™ðŸ„¶ðŸ„¸ðŸ……ðŸ„´ðŸ…ðŸ…‚ ðŸ„¶ðŸ„°ðŸ„¸ðŸ„½ â€¢ ðŸ…†ðŸ„¸ðŸ…‚ðŸ„³ðŸ„¾ðŸ„¼ âˆ™ ðŸ„²ðŸ„¾ðŸ„¼ðŸ„¿ðŸ„°ðŸ…‚ðŸ…‚ðŸ„¸ðŸ„¾ðŸ„½ â€¢\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " â™¡ â™¡ â™¡\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ð˜§ð˜³ð˜ªð˜¦ð˜¯ð˜¥ð˜­ð˜º ð˜¯ð˜¦ð˜ªð˜¨ð˜©ð˜£ð˜°ð˜¶ð˜³ð˜©ð˜°ð˜°ð˜¥ ð˜±ð˜¢ð˜¯ð˜´ð˜¦ð˜¹ð˜¶ð˜¢ð˜­ ð˜§ð˜ªð˜¨ð˜©ð˜µð˜ªð˜¯ð˜¨ ð˜µð˜©ð˜¦ ð˜¦ð˜®ð˜±ð˜ªð˜³ð˜¦ ð˜¢ð˜´ ð˜±ð˜¢ð˜³ð˜µ ð˜°ð˜§ ð˜µð˜©ð˜¦ ð˜³ð˜¦ð˜£ð˜¦ð˜­ ð˜¢ð˜­ð˜­ð˜ªð˜¢ð˜¯ð˜¤ð˜¦\n",
      " \n",
      "\n",
      "\n",
      "â˜¾ ðšœðš•ðšŽðšŽðš™ðš¢ ðšðš’ðš›ðš• ðš‹ðšžðšœðš¢ ðš•ðš’ðšðšŽ ðšƒðš‘ðšŽ ðšžðš—ðš˜ðšðšðš’ðšŒðš’ðšŠðš• ðšƒðš ðš’ðšðšðšŽðš› ðš˜ðš ð™°ðš—ðšðšŽðš• ðšˆðšžðšðš’ðšŽðš• ðšƒðš˜ðš›ðš›ðšŽðšœ ðš†ðš†ð™°ðš‚ â¥\n",
      "â‹¯â‹¯\n",
      "\n",
      "ð™°ðš—ðš ðš’ðšâ€™ ðš‹ðšŽðšŽðš— ðšŠðšðšŽðšœ ðšðš’ðšðšðšŽðš›ðšŽðš—ðš ðšœðšðšŠðšðšŽðšœ ðšŒðš˜ðš–ðšŽ ðšœðš˜ ðšðšŠðš› ðšðš›ðš˜ðš– ð™¿ðš›ðš’ðš—ðšŒðšŽðšœðšœ ð™¿ðšŠðš›ðš” ð™›ð™–ð™£ ð™–ð™˜ð™˜ ð™¨ð™ð™šð™ð™šð™§\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â € â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €\n",
      " \n",
      "â˜†\n",
      " ðŸ…³ðŸ…¸ðŸ…´ ðŸ†ƒðŸ†ðŸ†ˆðŸ…¸ðŸ…½ðŸ…¶\n",
      " \n",
      "\n",
      "ðœð¡ð¨ð¨ð¬ðž ð¥ð¨ð¯ðž\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "âš¢ â™¡\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "âœ¶âœ¶âœ¶ âœ¶ âœ¶ âœ¶âœ¶ âœ¶âœ¶ âœ¶\n",
      "\n",
      "\n",
      "\n",
      "ð”šð”¢ð”©ð” ð”¬ð”ªð”¢ ð”±ð”¬ ð”ªð”¶ ð”¥ð”¬ð”²ð”°ð”¢ â„­ð”¬ð”ªð”¢ ð”£ð”¯ð”¢ð”¢ð”©ð”¶ ð”Šð”¬ ð”°ð”žð”£ð”¢ð”©ð”¶ ð”ð”¢ð”žð”³ð”¢ ð”°ð”¬ð”ªð”¢ð”±ð”¥ð”¦ð”«ð”¤ ð”¬ð”£ ð”±ð”¥ð”¢ ð”¥ð”žð”­ð”­ð”¦ð”«ð”¢ð”°ð”° ð”¶ð”¬ð”² ð”Ÿð”¯ð”¦ð”«ð”¤\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â““â“â“¨â““â“¡â“”â“â“œ\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "ÂÂÂ ÂÂÂ ÂÂÂ Â ÂÂÂ Ë—ËË‹ \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " âœ˜ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â€¦\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "â•®â•¯â–½â•°â•­\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ð™²ðšŠðš•ðš– ð™»ðš’ðš”ðšŽ ð™±ðš˜ðš–ðš‹\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â €\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ð—˜ð—»ð˜ð—²ð—¿ð˜ð—®ð—¶ð—»ð—²ð—¿ ð—§ð—¿ð—®ð—»ð˜€ð—³ð—¼ð—¿ð—ºð—®ð˜ð—¶ð—¼ð—» ð—”ð—¿ð˜ð—¶ð˜€ð˜ ð—–ð—²ð—¹ð—²ð—¯ð—¿ð—¶ð˜ð˜† ð—œð—»ð—»ð—²ð—¿ ð—ªð—¼ð—¿ð—¹ð—± ð—˜ð˜…ð—½ð—²ð—¿ð˜ \n",
      "\n",
      "\n",
      "\n",
      "ð’•ð’‘ð’˜ð’Œ ð’ƒð’Šð’•ð’„ð’‰ð’†ð’”\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "â€™ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ï£¿\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "â€â€â€\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "â€” ðš’ðšœðš—â€™ ðšœðš‘ðšŽ ðšðš›ðšŽðšŠðš– ðšŒðš˜ðš–ðšŽ ðšðš›ðšžðšŽ \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "ð—¬ð—¼ð˜‚ð—§ð˜‚ð—¯ð—² ð—»ð—¶ð—°ð—¼ð—»ð—¶ð—°ð—¼ \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "ð“…ð’¶ð“‡ð“‰ ð“‰ð’¾ð“‚ð‘’ ð“Œð’¾ð“‰ð’¸ð’½ \n",
      "\n",
      "\n",
      " ð™‡ð™žð™œð™ð™© ð™¢ð™š ð™ªð™¥ ð™žð™£ ð™›ð™¡ð™–ð™¢ð™šð™¨ \n",
      " \n",
      "\n",
      " ã€âˆ \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "â€â€â€Žâ€Žâ€Žâ€Žâ€Žâ€Ž â€Žâ€Žâ€Ž\n",
      "\n",
      "â””\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " âƒŸÍŸâƒŸ \n",
      "\n",
      "ð•¯ð–”ð–’ð–Žð–“ð–Žð–ˆð–†ð–“ð–† â˜¼ â˜½ â†‘\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "â™¡\n",
      "Â¯Â¯ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "ð“¢ð“¬ð“»ð“®ð“®ð“·ð”€ð“»ð“²ð“½ð“®ð“» ð“œð“¸ð“¼ð“½ ð“½ð”€ð“®ð“®ð“½ð“¼ ð“ªð“»ð“® ð“ªð“«ð“¸ð“¾ð“½ ð“¯ð“²ð“µð“¶ ð“½ð“®ð“µð“®ð“¿ð“²ð“¼ð“²ð“¸ð“· ð“¿ð“²ð“­ð“®ð“¸ ð“°ð“ªð“¶ð“®ð“¼ ð“«ð“¸ð“¸ð“´ð“¼ ð“—ð“®ð“—ð“²ð“¶\n",
      "\n",
      "\n",
      "\n",
      "ï£¿ \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "ðšðšŠðš— ðšŠðšŒðšŒðš˜ðšžðš—ðš â™¡ðšœðš‘ðšŽðš‘ðšŽðš›\n",
      " \n",
      "\n",
      " \n",
      "ð•€ð•Ÿð•¤ð•¥ð•’ð•˜ð•£ð•’ð•ž ðš£ðšžðš›ðš’ðšŠðšŸðšŸðšŽðšðšŠ ð•”ð• ð•Ÿð•¥ð•’ð•”ð•¥ð•  \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "ðŸðšð­ð¡ðžð« ð¨ðŸ ðŸð¨ð®ð« ðŸð«ð¢ðžð§ð ð¨ðŸ ð¥ð¢ð›ð«ðšð«ð¢ðšð§ ðœðšð­ð¬\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "ð—¥ð—œð—¦ð—˜ð—¥ð—œð—¦ð—žð—¥ð—˜ð—£ð—˜ð—”ð—§ \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop through each description, performing POS tagging, stopword removal, and lemmatization\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "  \n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "x=['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
    "for description in descriptions:\n",
    "    # create list of stopwords to remove\n",
    "    C=detector.detect_language_of(description)\n",
    "    try :\n",
    "        if C.name.lower() in x:\n",
    "            stop_words = set(stopwords.words(C.name.lower()))\n",
    "        else :\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "    except :\n",
    "            print(description)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "    # perform tokenization\n",
    "    tokens = word_tokenize(description.lower())\n",
    "\n",
    "    # perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # initialize list of lemmatized tokens for this description\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    # loop through POS tagged tokens, removing certain parts of speech and lemmatizing others\n",
    "    for token in tokens:\n",
    "        # if pos.startswith('N') or pos.startswith('J'):\n",
    "            # keep nouns and adjectives, and lemmatize them\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if lemma not in stop_words:\n",
    "                lemmatized_tokens.append(lemma)\n",
    "\n",
    "    # add lemmatized tokens for this description to list\n",
    "    lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized_tokens_list.to_json('lemmatized_tokens_list.json', orient='records')\n",
    "with open('lemmatized_tokens_list.json', 'w') as f:\n",
    "    json.dump(lemmatized_tokens_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'zbb' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m----> 7\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m      8\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     10\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m----> 7\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m      8\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     10\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'zbb' not present\""
     ]
    }
   ],
   "source": [
    "# train Word2Vec model on all lemmatized tokens\n",
    "model = Word2Vec(lemmatized_tokens_list, min_count=5)\n",
    "\n",
    "# get vector representation of each description by averaging the vectors of its constituent lemmatized tokens\n",
    "description_vectors = []\n",
    "for lemmatized_tokens in lemmatized_tokens_list:\n",
    "    description_vector = sum(model.wv[token] for token in lemmatized_tokens) / len(lemmatized_tokens)\n",
    "    description_vectors.append(description_vector)\n",
    "\n",
    "# print vector representation of each description\n",
    "for i, vector in enumerate(description_vectors):\n",
    "    print(f\"Description {i+1} vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['he', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'he', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "Stem_words = []\n",
    "ps =PorterStemmer()\n",
    "for w in filtered_sentence:\n",
    "    rootWord=ps.stem(w)\n",
    "    Stem_words.append(rootWord)\n",
    "print(filtered_sentence)\n",
    "print(Stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROMANIAN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "C=detector.detect_language_of(\"teste teste\")\n",
    "C.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b269af9bc916de9a69a9129747e30eb936050c74c311d5baf33ccd4b86d3e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

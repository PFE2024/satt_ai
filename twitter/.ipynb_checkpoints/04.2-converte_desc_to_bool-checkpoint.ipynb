{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Data/data+column_relation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile_description  am bot that tweets about sports and technology follow me for updates\n",
      "doc  am bot that tweets about sports and technology follow me for updates\n",
      "[' ', 'bot', 'tweet', 'sport', 'technology', 'follow', 'update']\n",
      "pos count Counter({'NOUN': 4, 'PRON': 2, 'VERB': 2, 'ADP': 2, 'SPACE': 1, 'AUX': 1, 'CCONJ': 1})\n",
      "Profile description: I am a bot that tweets about sports and technology. Follow me for updates!\n",
      "Is bot? False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def is_bot_profile(profile_description):\n",
    "    # remove emojis\n",
    "    profile_description = TwitterPreprocessor(str(profile_description)).desc_preprocess().text\n",
    "    print(\"profile_description\",profile_description)\n",
    "    # tokenize the text with spaCy\n",
    "    doc = nlp(profile_description)\n",
    "    print(\"doc\",doc)\n",
    "    # remove stop words and punctuation\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "    print(tokens)\n",
    "    # count the frequency of different parts-of-speech\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    pos_counts = Counter(pos_tags)\n",
    "    print(\"pos count\",pos_counts)\n",
    "    # look for patterns typical of bot accounts\n",
    "    if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "        return True\n",
    "    elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#  \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "profile_description =\"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "is_bot = is_bot_profile(profile_description)\n",
    "print(f\"Profile description: {profile_description}\")\n",
    "print(f\"Is bot? {is_bot}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a bot that tweets about sports and technology. follow me for updates!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'spacy.morphology.Morphology' object has no attribute 'tag_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m profile_description \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mI am a bot that tweets about sports and technology. Follow me for updates!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m is_bot \u001b[39m=\u001b[39m is_bot_profile(profile_description)\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProfile description: \u001b[39m\u001b[39m{\u001b[39;00mprofile_description\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIs bot? \u001b[39m\u001b[39m{\u001b[39;00mis_bot\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mis_bot_profile\u001b[1;34m(profile_description)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mmorphology\u001b[39m.\u001b[39mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m token\u001b[39m.\u001b[39mlemma_ \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m \u001b[39m# count the frequency of different parts-of-speech\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m pos_tags \u001b[39m=\u001b[39m [nlp\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mmorphology\u001b[39m.\u001b[39;49mtag_map[token\u001b[39m.\u001b[39mtag_][\u001b[39m'\u001b[39m\u001b[39mPOS\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m nlp(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens))]\n\u001b[0;32m     20\u001b[0m pos_counts \u001b[39m=\u001b[39m Counter(pos_tags)\n\u001b[0;32m     21\u001b[0m \u001b[39m# look for patterns typical of bot accounts\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.morphology.Morphology' object has no attribute 'tag_map'"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# import emoji\n",
    "# from nltk.corpus import stopwords\n",
    "# from collections import Counter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def is_bot_profile(profile_description):\n",
    "#     # remove emojis\n",
    "#     # profile_description = emoji.get_emoji_regexp().sub(r'', profile_description)\n",
    "#     # lowercase the text\n",
    "#     profile_description = profile_description.lower()\n",
    "#     # tokenize the text with spaCy\n",
    "#     doc = nlp(profile_description)\n",
    "#     print(doc)\n",
    "#     # remove stop words and punctuation\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop and token.lemma_ not in stop_words]\n",
    "#     print(tokens)\n",
    "#     # count the frequency of different parts-of-speech\n",
    "#     pos_tags = [token.pos_ for token in doc]\n",
    "#     print(pos_tags)\n",
    "#     pos_counts = Counter(pos_tags)\n",
    "#     # look for patterns typical of bot accounts\n",
    "#     if pos_counts['PRON'] >= 3 and pos_counts['NOUN'] >= 3:\n",
    "#         return True\n",
    "#     elif pos_counts['VERB'] >= 2 and pos_counts['ADJ'] >= 2:\n",
    "#         return True\n",
    "#     else:\n",
    "#         return False\n",
    "# profile_description = \"I am a bot that tweets about sports and technology. Follow me for updates!\"\n",
    "# is_bot = is_bot_profile(profile_description)\n",
    "# print(f\"Profile description: {profile_description}\")\n",
    "# print(f\"Is bot? {is_bot}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chams\\AppData\\Local\\Temp\\ipykernel_16676\\774891700.py:7: DtypeWarning: Columns (0,3,4,14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# import spacy\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # Load the dataset\n",
    "# dataset = pd.read_csv(\"./Data/datafromAPI.csv\")\n",
    "\n",
    "# # Clean the text data\n",
    "# dataset['description'] = dataset['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "# dataset['description'] = dataset['description'].fillna('')\n",
    "# # Define function to lemmatize the tokens\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# def lemmatize_tokens(tokens):\n",
    "#     doc = nlp(\" \".join(tokens))\n",
    "#     return [token.lemma_ for token in doc]\n",
    "\n",
    "# # Tokenize and lemmatize the text data\n",
    "# dataset['description_tokens'] = dataset['description'].apply(lambda x: lemmatize_tokens(word_tokenize(x)))\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# dataset.to_csv('./Data/description_tokens.csv', index=False)\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Convert the list of tokens to a string\n",
    "# dataset['description_tokens'] = dataset['description_tokens'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add the dense array to the dataset\n",
    "# dataset['description_tokens'] = X_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_csv('./Data/description_tokensv2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start prepare desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "# # Remove irrelevant or duplicated data\n",
    "# df = df.drop(['description',  \n",
    "#          'account_type'], axis=1)\n",
    "\n",
    "# # Remove special characters and punctuation, and convert text to lowercase\n",
    "# df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).desc_preprocess().text)\n",
    "\n",
    "# # Tokenize text\n",
    "# df['description'] = df['description'].apply(lambda x: x.split())\n",
    "\n",
    "# # Remove stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# df['description'] = df['description'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# # Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# df['description'] = df['description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# # Convert back to string\n",
    "# df['description'] = df['description'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# # Split into training and testing sets\n",
    "# train_size = int(0.8 * len(df))\n",
    "# train_data = df[:train_size]\n",
    "# test_data = df[train_size:]\n",
    "\n",
    "# # Tokenize text inputs\n",
    "# tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(train_data['description'])\n",
    "\n",
    "# # Convert text inputs to sequences of integers\n",
    "# train_sequences = tokenizer.texts_to_sequences(train_data['description'])\n",
    "# test_sequences = tokenizer.texts_to_sequences(test_data['description'])\n",
    "\n",
    "# # Pad sequences\n",
    "# max_len = 100\n",
    "# train_data = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "# test_data = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# # Build model\n",
    "# model = Sequential([\n",
    "#     Embedding(10000, 128, input_length=max_len),\n",
    "#     LSTM(64),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dropout(0.5),\n",
    "#     Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "# # Compile model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Fit model\n",
    "# history = model.fit(train_data, train_data['label'], epochs=10, validation_data=(test_data, test_data['label']))\n",
    "\n",
    "# # Evaluate model\n",
    "# loss, accuracy = model.evaluate(test_data, test_data['label'])\n",
    "# print(f'Test loss: {loss:.4f}')\n",
    "# print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        television producer emmy award winner disney e...\n",
       "2                                          self discovery \n",
       "4        productor de televisión embajador de contacto ...\n",
       "6        im dmstic engineer married to hnk for yrs pups...\n",
       "7        shenanigatrix wordsmith events social media co...\n",
       "                               ...                        \n",
       "34039    self made plus model ceo siswim world traveler...\n",
       "34040                                   gazetecijournalist\n",
       "34041    modelo actriz venezolana venezuelan model actr...\n",
       "34042                              indianactorneed no more\n",
       "34043    poeta autor escritor acadêmico dois livros pub...\n",
       "Name: description, Length: 27034, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "\n",
    "# download required NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n",
    "\n",
    "\n",
    "# Remove special characters and punctuation, and convert text to lowercase\n",
    "df['description'] = df['description'].apply(lambda x :TwitterPreprocessor(str(x)).partially_preprocess().text)\n",
    "df = df.dropna(subset=['description'])\n",
    "# example list of string descriptions\n",
    "descriptions = df['description'] \n",
    "\n",
    "# create list of stopwords to remove\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# create WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize list of lemmatized tokens for each description\n",
    "lemmatized_tokens_list = []\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "\n",
    "\n",
    "descriptions.replace(\"nan\", nan_value, inplace=True)\n",
    "descriptions=descriptions.dropna()\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "¯¯\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "𝙄𝙣𝙨𝙩𝙖𝙜𝙧𝙖𝙢 𝙤𝙡𝙖𝙟𝙤𝙧𝙙𝙖𝙣 • 𝙎𝙣𝙖𝙥𝙘𝙝𝙖𝙩 𝙤𝙡𝙖𝙟𝙤𝙧𝙙𝙖𝙣 • 𝙈𝙚𝙙𝙞𝙖 𝙚𝙣𝙦𝙪𝙞𝙧𝙞𝙚𝙨 𝙘𝙤𝙣𝙩𝙖𝙘𝙩 \n",
      " \n",
      "\n",
      " \n",
      "𝘊𝘩𝘢𝘮𝘱𝘪𝘰𝘯 𝘥𝘶 𝘔𝘰𝘯𝘥𝘦 𝘍𝘊 𝘉𝘢𝘳𝘤𝘦𝘭𝘰𝘯𝘢 𝘤𝘰𝘯𝘵𝘢𝘤𝘵𝘤𝘰𝘮\n",
      " \n",
      "\n",
      "“𝑳𝒆𝒕𝒕𝒊𝒏𝒈 𝒑𝒆𝒐𝒑𝒍𝒆 𝒅𝒐𝒘𝒏 𝒊𝒔 𝒎𝒚 𝒕𝒉𝒊𝒏𝒈 𝒃𝒂𝒃𝒚 — 𝒇𝒊𝒏𝒅 𝒚𝒐𝒖𝒓𝒔𝒆𝒍𝒇 𝒏𝒆𝒘 𝒈𝒊𝒈” \n",
      "\n",
      "\n",
      "𝗕𝗲𝗛𝗮𝗽𝗽𝘆𝗜𝘁𝗗𝗿𝗶𝘃𝗲𝘀𝗣𝗲𝗼𝗽𝗹𝗲𝗖𝗿𝗮𝘇𝘆 \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " ♡\n",
      "\n",
      " \n",
      "☆\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " ‐ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ั\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ☥ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "◞‸◟☞ 𝙷𝚎𝚊𝚟𝚎𝚗’ 𝙶𝚊𝚝𝚎 𝚍𝚛𝚘𝚙𝚘𝚞𝚝 \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "••\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "•• \n",
      "\n",
      " ◠‿◠ \n",
      " \n",
      "\n",
      " \n",
      " 𝔱𝔥𝔢 𝔬𝔫𝔢 𝔞𝔫𝔡 𝔬𝔫𝔩𝔶 𝔩𝔦𝔳𝔦𝔫𝔤 𝔡𝔢𝔠𝔢𝔞𝔰𝔢𝔡 \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "𝒯𝒽ℯ 𝒟𝒾𝓋𝒾𝓃ℯ ℱℯ𝓂𝒾𝓃𝒾𝓃ℯ ❥\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "☆。\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "𝐀𝐓𝐋𝐀𝐒 𝐀𝐥𝐢𝐬 \n",
      "\n",
      "\n",
      "♡♡\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "𝑅𝑒𝓈𝒾𝓈𝓉𝒶𝓃𝒸𝑒 𝐿𝒾𝒷𝑒𝓇𝒶𝓁 𝒟𝑒𝓂 𝐵𝓁𝓊𝑒 𝒲𝒶𝓋𝑒 𝒮𝓃𝑜𝓌𝒻𝓁𝒶𝓀𝑒 𝒱𝑜𝓉𝑒𝐵𝓁𝓊𝑒𝒩𝑜𝑀𝒶𝓉𝓉𝑒𝓇𝒲𝒽𝑜 𝒟𝑀’\n",
      "\n",
      "☆ \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "𝕽𝖊𝖆𝖑 𝖎𝖘 𝕽𝖆𝖗𝖊 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "†\n",
      "\n",
      "\n",
      "¯¯\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ｅｌｅｃｔｉｏｎ ｃｏｖｅｒａｇｅ\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "△⃒⃘ ⚯͛ • • \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "！！！\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "♡ ♡\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "♡\n",
      "𝐓𝐡𝐞 𝐖𝐚𝐥𝐤𝐢𝐧𝐠 𝐃𝐞𝐚𝐝 𝐂𝐀𝐑𝐘𝐋 𝐺𝑒𝑡 𝑜𝑛 𝑡ℎ𝑒 𝑏𝑖𝑘𝑒 𝑎𝑛𝑑 𝑔𝑜 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "𝖌𝖔𝖙𝖍 𝖆𝖓𝖌𝖊𝖑 𝖘𝖎𝖓𝖓𝖊𝖗\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "｡･･★｡\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "𝑃𝑣𝑟𝑝𝑙𝑒𝑀𝑥𝑏𝐿𝑙𝑐 𝑬𝒔𝗍 𝟐𝟎𝟏𝟐\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "𝗀𝗂𝗋𝗅 𝖻𝗈𝗌𝗌 𝗌𝗍𝗈𝗋𝗒𝗅𝗂𝗇𝖾 𝗃𝖾𝗐𝖾𝗅𝗋𝗒\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "𝖨𝖦 𝖽𝗋𝖾𝖿𝖺𝖼𝖾 • 𝖲𝖢 𝖠𝗇𝖽𝗋𝖾𝖺 𝖣𝖺𝗏𝗂𝗌 • 𝖵𝗂𝗋𝗀𝗈 • • \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "࿊\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "𝓕𝓪𝓼𝓱𝓲𝓸𝓷 𝓓𝓮𝓼𝓲𝓰𝓷𝓮𝓻\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "𝐂𝐡𝐥𝐨𝐞 𝐈𝐧𝐝𝐢𝐚\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "✭ ☆\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 𝕕𝕠𝕟𝕥 𝕣𝕒𝕡 𝕛𝕦𝕤𝕥 𝕥𝕒𝕝𝕜 𝕝𝕠𝕥 𝕒𝕟𝕕 𝕤𝕠𝕞𝕖𝕥𝕚𝕞𝕖𝕤 𝕚𝕥 𝕘𝕖𝕥𝕤 𝕣𝕖𝕔𝕠𝕣𝕕𝕖𝕕\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "ⓥ \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "— 𝑎𝑛𝑖𝑠𝑡𝑜𝑛 𝑑𝑒𝑚𝑝𝑠𝑒𝑦 𝑝𝑜𝑚𝑝𝑒𝑜⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀ ⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀ 𝑓𝑎𝑛 𝑎𝑐𝑐𝑜𝑢𝑛𝑡\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "𝑴𝒂𝒌𝒆𝒖𝒑 𝑨𝒓𝒕𝒊𝒔𝒕 𝑷𝒉𝒐𝒕𝒐𝒈𝒓𝒂𝒑𝒉𝒆𝒓\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "｀∇´\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "☉⭡ ☽ \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "𝙏𝙑 𝙋𝙧𝙤𝙙𝙪𝙘𝙚𝙧 𝙋𝙤𝙙𝙘𝙖𝙨𝙩 𝙃𝙤𝙨𝙩 𝙤𝙛 𝙅𝙤𝙣𝙖𝙝 𝙖𝙣𝙙 𝙩𝙝𝙚 𝙒𝙝𝙖𝙡𝙚\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "𝐈𝐆 𝐅𝐁 𝐂𝐫𝐢𝐬𝐭𝐢𝐧𝐞 𝐑𝐞𝐲𝐞𝐬 ∙🄶🄸🅅🄴🅁🅂 🄶🄰🄸🄽 • 🅆🄸🅂🄳🄾🄼 ∙ 🄲🄾🄼🄿🄰🅂🅂🄸🄾🄽 •\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " ♡ ♡ ♡\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "𝘧𝘳𝘪𝘦𝘯𝘥𝘭𝘺 𝘯𝘦𝘪𝘨𝘩𝘣𝘰𝘶𝘳𝘩𝘰𝘰𝘥 𝘱𝘢𝘯𝘴𝘦𝘹𝘶𝘢𝘭 𝘧𝘪𝘨𝘩𝘵𝘪𝘯𝘨 𝘵𝘩𝘦 𝘦𝘮𝘱𝘪𝘳𝘦 𝘢𝘴 𝘱𝘢𝘳𝘵 𝘰𝘧 𝘵𝘩𝘦 𝘳𝘦𝘣𝘦𝘭 𝘢𝘭𝘭𝘪𝘢𝘯𝘤𝘦\n",
      " \n",
      "\n",
      "\n",
      "☾ 𝚜𝚕𝚎𝚎𝚙𝚢 𝚐𝚒𝚛𝚕 𝚋𝚞𝚜𝚢 𝚕𝚒𝚏𝚎 𝚃𝚑𝚎 𝚞𝚗𝚘𝚏𝚏𝚒𝚌𝚒𝚊𝚕 𝚃𝚠𝚒𝚝𝚝𝚎𝚛 𝚘𝚏 𝙰𝚗𝚐𝚎𝚕 𝚈𝚞𝚍𝚒𝚎𝚕 𝚃𝚘𝚛𝚛𝚎𝚜 𝚆𝚆𝙰𝚂 ❥\n",
      "⋯⋯\n",
      "\n",
      "𝙰𝚗𝚍 𝚒𝚝’ 𝚋𝚎𝚎𝚗 𝚊𝚐𝚎𝚜 𝚍𝚒𝚏𝚏𝚎𝚛𝚎𝚗𝚝 𝚜𝚝𝚊𝚐𝚎𝚜 𝚌𝚘𝚖𝚎 𝚜𝚘 𝚏𝚊𝚛 𝚏𝚛𝚘𝚖 𝙿𝚛𝚒𝚗𝚌𝚎𝚜𝚜 𝙿𝚊𝚛𝚔 𝙛𝙖𝙣 𝙖𝙘𝙘 𝙨𝙝𝙚𝙝𝙚𝙧\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
      " \n",
      "☆\n",
      " 🅳🅸🅴 🆃🆁🆈🅸🅽🅶\n",
      " \n",
      "\n",
      "𝐜𝐡𝐨𝐨𝐬𝐞 𝐥𝐨𝐯𝐞\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "⚢ ♡\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "✶✶✶ ✶ ✶ ✶✶ ✶✶ ✶\n",
      "\n",
      "\n",
      "\n",
      "𝔚𝔢𝔩𝔠𝔬𝔪𝔢 𝔱𝔬 𝔪𝔶 𝔥𝔬𝔲𝔰𝔢 ℭ𝔬𝔪𝔢 𝔣𝔯𝔢𝔢𝔩𝔶 𝔊𝔬 𝔰𝔞𝔣𝔢𝔩𝔶 𝔏𝔢𝔞𝔳𝔢 𝔰𝔬𝔪𝔢𝔱𝔥𝔦𝔫𝔤 𝔬𝔣 𝔱𝔥𝔢 𝔥𝔞𝔭𝔭𝔦𝔫𝔢𝔰𝔰 𝔶𝔬𝔲 𝔟𝔯𝔦𝔫𝔤\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ⓓⓐⓨⓓⓡⓔⓐⓜ\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "     ˗ˏˋ \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ✘ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "…\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "╮╯▽╰╭\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "𝙲𝚊𝚕𝚖 𝙻𝚒𝚔𝚎 𝙱𝚘𝚖𝚋\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "⠀\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "𝗘𝗻𝘁𝗲𝗿𝘁𝗮𝗶𝗻𝗲𝗿 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝗮𝘁𝗶𝗼𝗻 𝗔𝗿𝘁𝗶𝘀𝘁 𝗖𝗲𝗹𝗲𝗯𝗿𝗶𝘁𝘆 𝗜𝗻𝗻𝗲𝗿 𝗪𝗼𝗿𝗹𝗱 𝗘𝘅𝗽𝗲𝗿𝘁 \n",
      "\n",
      "\n",
      "\n",
      "𝒕𝒑𝒘𝒌 𝒃𝒊𝒕𝒄𝒉𝒆𝒔\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "’ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "‏‏‏\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "— 𝚒𝚜𝚗’ 𝚜𝚑𝚎 𝚍𝚛𝚎𝚊𝚖 𝚌𝚘𝚖𝚎 𝚝𝚛𝚞𝚎 \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "𝗬𝗼𝘂𝗧𝘂𝗯𝗲 𝗻𝗶𝗰𝗼𝗻𝗶𝗰𝗼 \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "𝓅𝒶𝓇𝓉 𝓉𝒾𝓂𝑒 𝓌𝒾𝓉𝒸𝒽 \n",
      "\n",
      "\n",
      " 𝙇𝙞𝙜𝙝𝙩 𝙢𝙚 𝙪𝙥 𝙞𝙣 𝙛𝙡𝙖𝙢𝙚𝙨 \n",
      " \n",
      "\n",
      " 」∠\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "‏‏‎‎‎‎‎‎ ‎‎‎\n",
      "\n",
      "└\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      " ⃟͟⃟ \n",
      "\n",
      "𝕯𝖔𝖒𝖎𝖓𝖎𝖈𝖆𝖓𝖆 ☼ ☽ ↑\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "♡\n",
      "¯¯ \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "𝓢𝓬𝓻𝓮𝓮𝓷𝔀𝓻𝓲𝓽𝓮𝓻 𝓜𝓸𝓼𝓽 𝓽𝔀𝓮𝓮𝓽𝓼 𝓪𝓻𝓮 𝓪𝓫𝓸𝓾𝓽 𝓯𝓲𝓵𝓶 𝓽𝓮𝓵𝓮𝓿𝓲𝓼𝓲𝓸𝓷 𝓿𝓲𝓭𝓮𝓸 𝓰𝓪𝓶𝓮𝓼 𝓫𝓸𝓸𝓴𝓼 𝓗𝓮𝓗𝓲𝓶\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "𝚏𝚊𝚗 𝚊𝚌𝚌𝚘𝚞𝚗𝚝 ♡𝚜𝚑𝚎𝚑𝚎𝚛\n",
      " \n",
      "\n",
      " \n",
      "𝕀𝕟𝕤𝕥𝕒𝕘𝕣𝕒𝕞 𝚣𝚞𝚛𝚒𝚊𝚟𝚟𝚎𝚐𝚊 𝕔𝕠𝕟𝕥𝕒𝕔𝕥𝕠 \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "𝐟𝐚𝐭𝐡𝐞𝐫 𝐨𝐟 𝐟𝐨𝐮𝐫 𝐟𝐫𝐢𝐞𝐧𝐝 𝐨𝐟 𝐥𝐢𝐛𝐫𝐚𝐫𝐢𝐚𝐧 𝐜𝐚𝐭𝐬\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "𝗥𝗜𝗦𝗘𝗥𝗜𝗦𝗞𝗥𝗘𝗣𝗘𝗔𝗧 \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop through each description, performing POS tagging, stopword removal, and lemmatization\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "  \n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "x=['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n",
    "for description in descriptions:\n",
    "    # create list of stopwords to remove\n",
    "    C=detector.detect_language_of(description)\n",
    "    try :\n",
    "        if C.name.lower() in x:\n",
    "            stop_words = set(stopwords.words(C.name.lower()))\n",
    "        else :\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "    except :\n",
    "            print(description)\n",
    "            stop_words = set(stopwords.words(\"english\"))\n",
    "    # perform tokenization\n",
    "    tokens = word_tokenize(description.lower())\n",
    "\n",
    "    # perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # initialize list of lemmatized tokens for this description\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    # loop through POS tagged tokens, removing certain parts of speech and lemmatizing others\n",
    "    for token in tokens:\n",
    "        # if pos.startswith('N') or pos.startswith('J'):\n",
    "            # keep nouns and adjectives, and lemmatize them\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            if lemma not in stop_words:\n",
    "                lemmatized_tokens.append(lemma)\n",
    "\n",
    "    # add lemmatized tokens for this description to list\n",
    "    lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatized_tokens_list.to_json('lemmatized_tokens_list.json', orient='records')\n",
    "with open('lemmatized_tokens_list.json', 'w') as f:\n",
    "    json.dump(lemmatized_tokens_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'zbb' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m----> 7\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39;49m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m      8\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     10\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m description_vectors \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m lemmatized_tokens \u001b[39min\u001b[39;00m lemmatized_tokens_list:\n\u001b[0;32m----> 7\u001b[0m     description_vector \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(model\u001b[39m.\u001b[39;49mwv[token] \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m lemmatized_tokens) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(lemmatized_tokens)\n\u001b[1;32m      8\u001b[0m     description_vectors\u001b[39m.\u001b[39mappend(description_vector)\n\u001b[1;32m     10\u001b[0m \u001b[39m# print vector representation of each description\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_vector(key_or_keys)\n\u001b[1;32m    405\u001b[0m \u001b[39mreturn\u001b[39;00m vstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_vector(key) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_vector\u001b[39m(\u001b[39mself\u001b[39m, key, norm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_index(key)\n\u001b[1;32m    447\u001b[0m     \u001b[39mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/Documents/AI/venv/lib/python3.10/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m not present\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'zbb' not present\""
     ]
    }
   ],
   "source": [
    "# train Word2Vec model on all lemmatized tokens\n",
    "model = Word2Vec(lemmatized_tokens_list, min_count=5)\n",
    "\n",
    "# get vector representation of each description by averaging the vectors of its constituent lemmatized tokens\n",
    "description_vectors = []\n",
    "for lemmatized_tokens in lemmatized_tokens_list:\n",
    "    description_vector = sum(model.wv[token] for token in lemmatized_tokens) / len(lemmatized_tokens)\n",
    "    description_vectors.append(description_vector)\n",
    "\n",
    "# print vector representation of each description\n",
    "for i, vector in enumerate(description_vectors):\n",
    "    print(f\"Description {i+1} vector: {vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from Twitterpreprocessor import TwitterPreprocessor\n",
    "df = pd.read_csv('./Data/data+descvf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['he', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'he', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(text) \n",
    "    \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "Stem_words = []\n",
    "ps =PorterStemmer()\n",
    "for w in filtered_sentence:\n",
    "    rootWord=ps.stem(w)\n",
    "    Stem_words.append(rootWord)\n",
    "print(filtered_sentence)\n",
    "print(Stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ROMANIAN'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.FRENCH, Language.GERMAN, Language.SPANISH]\n",
    "detector = LanguageDetectorBuilder.from_all_languages().build()\n",
    "C=detector.detect_language_of(\"teste teste\")\n",
    "C.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim.downloader as api\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "919f1a12e43db5d5accd328457ee16aa753b873ae7a9cf7fbc94ca5438fe0a95"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
